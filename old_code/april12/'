import numpy as np
import pandas as pd
import scipy.special
from scipy.linalg import sqrtm
import scipy.stats as stats
import numpy_indexed as npi

from helper_functions import clip, var2suffvec, suffvec2var, get_utri, symmetric_fill_utri 
from least_squares_helper import get_est_eqn_LS
from synthetic_env import make_base_study_df
#get_est_eqn_posterior_sampling

from helper_functions import PRECISION
#NUM_POSTERIOR_SAMPLES=10000
NUM_POSTERIOR_SAMPLES=2000

def generalized_logistic(args, lin_est):
    inner = args.steepness*lin_est/args.allocation_sigma
    raw = scipy.special.expit( inner )
    pis = args.lower_clip + (args.upper_clip-args.lower_clip) * raw
    return pis
    

class SmoothPosteriorSampling:
    """
    Smooth posterior algorithm
    """
    def __init__(self, args, state_feats, treat_feats, alg_seed, allocation_sigma, 
                 steepness, prior_mean, prior_var, noise_var, action_centering):
        self.args = args
        self.state_feats = state_feats
        self.treat_feats = treat_feats
        self.alg_seed = alg_seed
        self.rng = np.random.default_rng(alg_seed)
        self.allocation_sigma = allocation_sigma
        self.steepness = steepness

        self.action_centering = action_centering
        if action_centering:
            total_dim = len(state_feats) + len(treat_feats)*2
        else:
            total_dim = len(state_feats) + len(treat_feats)
        self.prior_mean = prior_mean
        self.prior_var = prior_var
        self.inv_prior_var = np.linalg.inv(prior_var)
        self.noise_var = noise_var

        var_suffvec = var2suffvec(self, prior_var)
        zero_var_suffvec = np.zeros( var_suffvec.shape )
        initial_beta_est = np.concatenate( [self.prior_mean, zero_var_suffvec] )

        alg_dict = {
            "policy_last_t": 0,
            "post_mean": self.prior_mean,
            "post_var": self.prior_var,
            "new_data" : None,
            "total_obs" : 0,
            "all_user_id": set(),
            "XX": np.zeros(self.prior_var.shape),
            "RX": np.zeros(self.prior_mean.shape),
            "all_user_id": set(),
            "est_params": initial_beta_est,
            "n_users": 1,
            "intercept_val": 0,
            # data used to get updated est_params
        }
        self.all_policies = [alg_dict]
        
        self.treat_feats_action = ['action:'+x for x in self.treat_feats]
        self.treat_bool = np.array( [True if x in self.treat_feats_action else 
                      False for x in self.state_feats+self.treat_feats_action ] )

        # used for forming action selection probabilities
        norm_samples_df = make_base_study_df(args)
        norm_samples = self.rng.normal(loc=0, scale=1,
                        size=(args.T*args.n, NUM_POSTERIOR_SAMPLES))
        #tmp_df = pd.DataFrame(norm_samples, 
        #            columns=[x for x in range(NUM_POSTERIOR_SAMPLES)])
        #self.norm_samples_df = pd.concat([norm_samples_df, tmp_df], 
        #                                 axis=1)
        self.norm_samples_df = norm_samples_df
        self.norm_samples = norm_samples
        
   
    def update_alg(self, new_data, t):
        # update algorithm with new data
        rewards = new_data['reward'].to_numpy().reshape(-1,1)
        actions = new_data['action'].to_numpy().reshape(-1,1)
        action1prob = new_data['action1prob'].to_numpy().reshape(-1,1)
        base_states, treat_states = self.get_states(new_data)

        if self.action_centering:
            design = np.concatenate( [ base_states, action1prob * treat_states,
                        (actions-action1prob) * treat_states ], axis=1 )
        else:
            design = np.concatenate( [ base_states, actions * treat_states ], axis=1 )
        
        # Only include available data
        calendar_t = new_data['calendar_t'].to_numpy().reshape(-1,1)
        user_t = new_data['user_t'].to_numpy().reshape(-1,1)
        user_id = new_data['user_id'].to_numpy()
        if self.args.dataset_type == 'heartsteps':
            avail_bool = new_data['availability'].astype('bool')
            rewards_avail = rewards[ avail_bool ]
            design_avail = design[ avail_bool ]
            user_id_avail = user_id[avail_bool]
            calendar_t = calendar_t[avail_bool]
            user_t = user_t[avail_bool]
        else:
            avail_bool = np.ones(rewards.shape)
            rewards_avail = rewards
            design_avail = design
            user_id_avail = user_id


        # Algorithm Update
        prev_policy_dict = self.all_policies[-1]
        RX = prev_policy_dict['RX'] + np.sum(rewards_avail * design_avail, axis=0)
        XX_add = np.einsum( 'ij,ik->jk', design_avail, design_avail )
        XX = prev_policy_dict['XX'] + XX_add
        inv_post_var = XX + np.linalg.inv( self.prior_var )
        post_var = np.linalg.inv( inv_post_var )
        prior_adj = np.matmul( np.linalg.inv(self.prior_var), self.prior_mean )
        post_mean = np.matmul( post_var, RX + prior_adj )

        col_names = self.state_feats+['action:'+x for x in self.treat_feats]
        all_user_id_set = prev_policy_dict["all_user_id"].copy()
        all_user_id_set.update( new_data["user_id"].to_numpy() )
        V_matrix = XX / len(all_user_id_set)

        # Save Data
        inc_data = {
                "reward": rewards.flatten(),
                "action": actions.flatten(),
                "action1prob": action1prob.flatten(),
                "base_states": base_states,
                "treat_states": treat_states,
                "avail": avail_bool.flatten(),
                "user_id": user_id_avail,
                "norm_samples": tmp_norm_samples, # TODO reshape?
            }
        alg_dict = {
            "policy_last_t": policy_last_t,
            "new_data" : new_data,  # data used to get updated est_params
            #"design" : design_avail,
            "total_obs" : self.all_policies[-1]['total_obs'] + len(new_data),
        }

        alg_dict["all_user_id"] = all_user_id_set
        alg_dict["n_users"] = len(all_user_id_set)
        alg_dict["post_mean"] = post_mean
        alg_dict["post_var"] = post_var
        alg_dict["col_names"] = col_names
        alg_dict["V_matrix"] = V_matrix
        alg_dict["XX"] = XX
        alg_dict["RX"] = RX
        alg_dict["intercept_val"] = V_matrix[0][0]
        
        V_suffvec = var2suffvec(self, V_matrix)
        #varmatrix = suffvec2var(self, post_V_params)
        #post_V_params = get_utri(post_V)
        
        est_params = np.hstack( [ post_mean, V_suffvec ] )
        alg_dict["est_params"] = est_params
        

        import ipdb; ipdb.set_trace()
        
        # Get noise that is saved
        selected_norm_idx = self.norm_samples_df['policy_last_t'] == policy_last_t
        tmp_norm_samples = self.norm_samples[selected_norm_idx]
        
        # Incremental data info
        alg_dict['inc_data'] = inc_data_dict
        #print(t, "rewards", rewards.shape)

        self.all_policies.append(alg_dict)


    def get_states(self, tmp_df):
        base_states = tmp_df[self.state_feats].to_numpy()
        treat_states = tmp_df[self.treat_feats].to_numpy()
        return (base_states, treat_states)


    def get_action_probs(self, curr_timestep_data, filter_keyval):
     
        """
        post_var = self.all_policies[-1]["post_var"]
        probs1 = self.get_action_probs_inner_old(curr_timestep_data, 
                beta_est = self.all_policies[-1]["est_params"], 
                n_users = self.all_policies[-1]["n_users"],
                check_post_var = post_var,
                intercept_val = self.all_policies[-1]['intercept_val'], 
                filter_keyval = filter_keyval)
        """
        
        post_var = self.all_policies[-1]["post_var"]
        base_states, treat_states = self.get_states(curr_timestep_data)
        probs = self.get_action_probs_inner(treat_states, 
                beta_est = self.all_policies[-1]["est_params"], 
                n_users = self.all_policies[-1]["n_users"],
                intercept_val = self.all_policies[-1]['intercept_val'], 
                filter_keyval = filter_keyval,
                check_post_var = post_var)
        
        return probs

    
    def get_action_probs_inner(self, treat_states, beta_est, n_users, 
                    intercept_val, filter_keyval, check_post_var=None):

        # Get posterior parameters
        if self.action_centering:
            state_dim = len(self.state_feats) + len(self.treat_feats)*2
        else:
            state_dim = len(self.state_feats) + len(self.treat_feats)
        
        post_mean = beta_est[:state_dim]
        V_suff_vector = beta_est[state_dim:]
        
        V_matrix = suffvec2var(self, V_suff_vector, intercept_val)
        #post_V = symmetric_fill_utri(post_V_pieces, post_mean.shape[0])
       
        # construct posterior variance
        alg_n_users = self.all_policies[-1]["n_users"]
        post_var = np.linalg.inv( V_matrix * alg_n_users + self.inv_prior_var )

        # Check reconstruction of posterior variance
        if check_post_var is not None:
            try:
                assert np.equal( np.around(post_var,3), 
                                 np.around(check_post_var,3) ).all()
            except:
                import ipdb; ipdb.set_trace()

        # Get posterior parameters for treatment effect (action selection)
        treat_dim = len(self.treat_feats)
        post_mean_treat = post_mean[-treat_dim:]
        post_var_treat = post_var[-treat_dim:,-treat_dim:]

        post_mean_user = np.einsum('ij,j->i', treat_states, post_mean_treat)
        post_var_user = np.einsum('ij,ij->i', np.matmul(treat_states, 
                                            post_var_treat), treat_states) 
        post_std_user = np.sqrt(post_var_user)

        ########### Form action selection probabilities (Sampling)
        
        # Get noise that is saved
        selected_norm_idx = self.norm_samples_df[filter_keyval[0]] == filter_keyval[1]
        tmp_norm_samples = self.norm_samples[selected_norm_idx]
        
        #selected_norm_df = self.norm_samples_df[selected_norm_idx]
        #tmp_norm_samples = selected_norm_df.to_numpy()[:,-NUM_POSTERIOR_SAMPLES:]
        
        n_tmp = len(post_mean_user)
        post_samples = tmp_norm_samples * post_std_user.reshape(n_tmp, -1) + \
                post_mean_user.reshape(n_tmp, -1)

        prob_samples = generalized_logistic(self.args, post_samples)
        probs = np.mean(prob_samples, axis=1)
        """
        # Form action selection probabilities (Integration)
        all_action1_probs = []
        n_tmp = len(post_mean_user)
        for i in range(n_tmp):
            action1_prob = stats.norm.expect(
                    func=lambda x: generalized_logistic(
                        self.args, x), 
                    loc = post_mean_user[i], 
                    scale = np.sqrt(post_var_user[i]) )
            all_action1_probs.append(action1_prob)
        probs = np.array(all_action1_probs)
        #if PRECISION is not None:
        #    probs = np.around(probs, PRECISION)
        """
        
        return probs
    

    def get_est_eqns(self, beta_params, data_dict, all_user_ids, 
                    data_sofar=None, return_ave_only=False, action1probs=None,
                    correction="", check=False, intercept_val=None, light=False):
        """
        Get estimating equations for policy parameters for one update
        """

        if self.action_centering:
            state_dim = len(self.state_feats) + len(self.treat_feats)*2
        else:
            state_dim = len(self.state_feats) + len(self.treat_feats)

        prior_dict = {
                "state_dim": state_dim,
                "prior_mean": self.prior_mean,
                "prior_var": self.prior_var,
                "noise_var": self.noise_var,
                }
        
        # Posterior Mean and V (using data dictionary) #####################
        outcome_vec = data_dict['rewards']
        actions = data_dict['actions'].reshape(-1,1)
        actions1probs = data_dict['action1probs']
        base_states = data_dict['base_states']
        treat_states = data_dict['treat_states']
        avail_vec = data_dict['avail'].flatten()
        user_ids = data_dict['user_ids']

        if self.action_centering:
            action1probs = action1probs.reshape(action1probs.shape[0], -1)
            design = np.concatenate( [ base_states, action1probs * treat_states,
                        (actions-action1probs) * treat_states ], axis=1 )
        else:
            design = np.concatenate( [ base_states, actions * treat_states ], axis=1 )

        est_eqn_dict = get_est_eqn_LS(outcome_vec, design, user_ids,
                                      beta_params, avail_vec, all_user_ids,
                                      prior_dict = prior_dict,
                                      correction = correction,
                                      reconstruct_check = check,
                                      RL_alg = self,
                                      intercept_val = intercept_val,
                                      light = light)
       
        #est_eqn_dict2['est_eqns'] == est_eqn_dict['est_eqns']
        
        if return_ave_only:
            return np.sum(est_eqn_dict['est_eqns'], axis=0) / len(user_ids)
        return est_eqn_dict




    def get_weights(self, beta_params, all_user_ids, 
                    collected_data_dict, filter_keyval, intercept_val, 
                    curr_policy_decision_data=None):
        """
        Get Radon Nikodym weights for all weights for all decisions made by a given policy update
        """
       
        action = collected_data_dict['actions']
        used_prob1 = collected_data_dict['action1probs']
        used_probA = action*used_prob1 + (1-action)*(1-used_prob1)
        treat_states = collected_data_dict['treat_states']

        prob1_beta = self.get_action_probs_inner(treat_states, beta_params, 
                                                 n_users=len(all_user_ids),
                                                 intercept_val=intercept_val,
                                                 filter_keyval=filter_keyval)
        probA_beta = action*prob1_beta + (1-action)*(1-prob1_beta) 
        weights_subset = probA_beta / used_probA

        # cluster by user id 
        pi_user_ids = collected_data_dict['user_ids']
        unique_pi_ids = set(pi_user_ids)
        add_users = set(all_user_ids) - unique_pi_ids
        if len(add_users) > 0:
            all_user_ids = np.concatenate( [[x for x in add_users], pi_user_ids] )
            ones = np.zeros((len(add_users), weights_subset.shape[1]))
            weights_subset = np.concatenate( [ones, weights_subset], axis=0 )

            sort_idx = np.argsort(pi_user_ids)
            pi_user_ids = pi_user_ids[sort_idx]
            weights_subset = weights_subset[sort_idx]

        user_pi_weights = npi.group_by(pi_user_ids).sum(weights_subset)[1]

        return user_pi_weights














    def get_est_eqns_df(self, beta_params, data_sofar, all_user_ids, 
                    return_ave_only=False, action1probs=None,
                    correction="", check=False, intercept_val=None):
        """
        Get estimating equations for policy parameters for one update
        """
       
        if self.action_centering:
            state_dim = len(self.state_feats) + len(self.treat_feats)*2
        else:
            state_dim = len(self.state_feats) + len(self.treat_feats)

        prior_dict = {
                "state_dim": state_dim,
                "prior_mean": self.prior_mean,
                "prior_var": self.prior_var,
                "noise_var": self.noise_var,
                }

        # Posterior Mean and V #################################
        actions = data_sofar.action.to_numpy().reshape(-1,1)
        if self.action_centering:
            action1probs = action1probs.reshape(action1probs.shape[0], -1)
            X_vecs = np.concatenate( [ data_sofar[self.state_feats],
                        action1probs * data_sofar[self.treat_feats],
                        (actions-action1probs) * data_sofar[self.treat_feats] ], axis=1 )
        else:
            X_vecs = np.concatenate( [ data_sofar[self.state_feats].to_numpy(),
                        actions * data_sofar[self.treat_feats].to_numpy() ], axis=1 )
        user_ids = data_sofar.user_id.to_numpy()
        outcome_vec = data_sofar.reward.to_numpy()
        design = X_vecs

        if self.args.dataset_type == 'heartsteps':
            avail_vec = data_sofar.availability.to_numpy()
        else:
            avail_vec = np.ones(outcome_vec.shape)


        est_eqn_dict = get_est_eqn_LS(outcome_vec, design, user_ids,
                                      beta_params, avail_vec, all_user_ids,
                                      prior_dict = prior_dict,
                                      correction = correction,
                                      reconstruct_check = check,
                                      RL_alg = self,
                                      intercept_val = intercept_val)
        
        if return_ave_only:
            return np.sum(est_eqn_dict['est_eqns'], axis=0) / len(user_ids)
        return est_eqn_dict




    def get_action_probs_old(self, curr_timestep_data, filter_keyval):
        post_var = self.all_policies[-1]["post_var"]
        probs = self.get_action_probs_inner_old(curr_timestep_data, 
                beta_est = self.all_policies[-1]["est_params"], 
                n_users = self.all_policies[-1]["n_users"],
                check_post_var = post_var,
                intercept_val = self.all_policies[-1]['intercept_val'], 
                filter_keyval = filter_keyval)

        return probs

    

    def get_action_probs_inner_old(self, curr_timestep_data, beta_est, n_users, 
                    intercept_val, filter_keyval, check_post_var=None):
        treat_states = curr_timestep_data[self.treat_feats].to_numpy()

        # Get posterior parameters
        if self.action_centering:
            state_dim = len(self.state_feats) + len(self.treat_feats)*2
        else:
            state_dim = len(self.state_feats) + len(self.treat_feats)
        
        post_mean = beta_est[:state_dim]
        V_suff_vector = beta_est[state_dim:]
        
        V_matrix = suffvec2var(self, V_suff_vector, intercept_val)
        #post_V = symmetric_fill_utri(post_V_pieces, post_mean.shape[0])
       
        # construct posterior variance
        prior_var_inv = np.linalg.inv( self.prior_var )
        alg_n_users = self.all_policies[-1]["n_users"]
        post_var = np.linalg.inv( V_matrix * alg_n_users + prior_var_inv )

        # Check reconstruction of posterior variance
        if check_post_var is not None:
            try:
                assert np.equal( np.around(post_var,3), 
                                 np.around(check_post_var,3) ).all()
            except:
                import ipdb; ipdb.set_trace()

        # Get posterior parameters for treatment effect (action selection)
        treat_dim = len(self.treat_feats)
        post_mean_treat = post_mean[-treat_dim:]
        post_var_treat = post_var[-treat_dim:,-treat_dim:]

        post_mean_user = np.einsum('ij,j->i', treat_states, post_mean_treat)
        post_var_user = np.einsum('ij,ij->i', np.matmul(treat_states, 
                                            post_var_treat), treat_states) 
        post_std_user = np.sqrt(post_var_user)

        ########### Form action selection probabilities (Sampling)
        
        # Get noise that is saved
        selected_norm_idx = self.norm_samples_df[filter_keyval[0]] == filter_keyval[1]
        tmp_norm_samples = self.norm_samples[selected_norm_idx]
        
        #selected_norm_df = self.norm_samples_df[selected_norm_idx]
        #tmp_norm_samples = selected_norm_df.to_numpy()[:,-NUM_POSTERIOR_SAMPLES:]
        
        n_tmp = len(post_mean_user)
        post_samples = tmp_norm_samples * post_std_user.reshape(n_tmp, -1) + \
                post_mean_user.reshape(n_tmp, -1)

        prob_samples = generalized_logistic(self.args, post_samples)
        probs = np.mean(prob_samples, axis=1)
        """
        # Form action selection probabilities (Integration)
        all_action1_probs = []
        n_tmp = len(post_mean_user)
        for i in range(n_tmp):
            action1_prob = stats.norm.expect(
                    func=lambda x: generalized_logistic(
                        self.args, x), 
                    loc = post_mean_user[i], 
                    scale = np.sqrt(post_var_user[i]) )
            all_action1_probs.append(action1_prob)
        probs = np.array(all_action1_probs)
        #if PRECISION is not None:
        #    probs = np.around(probs, PRECISION)
        """
        
        return probs
    

    def get_weights_old(self, curr_policy_decision_data, beta_params, all_user_ids, filter_keyval, intercept_val):
        """
        Get Radon Nikodym weights for all weights for all decisions made by a given policy update
        """
        
        action = curr_policy_decision_data['action'].to_numpy()
        
        used_prob1 = curr_policy_decision_data['action1prob'].to_numpy()
        used_probA = action*used_prob1 + (1-action)*(1-used_prob1)

        prob1_beta = self.get_action_probs_inner_old(curr_policy_decision_data, beta_params, 
                                                 n_users=len(all_user_ids),
                                                 intercept_val=intercept_val,
                                                 filter_keyval=filter_keyval)
        probA_beta = action*prob1_beta + (1-action)*(1-prob1_beta) 

        weights_subset = probA_beta / used_probA

        # cluster by user id 
        pi_user_ids = curr_policy_decision_data['user_id'].to_numpy()
        unique_pi_ids = np.unique(pi_user_ids)
        user_pi_weights = []
        for idx in all_user_ids:
            if idx in pi_user_ids:
                tmp_weight = np.prod( weights_subset[ pi_user_ids == idx ], axis=0 )
            else:
                tmp_weight = 1
            user_pi_weights.append( tmp_weight )
        user_pi_weights = np.array( user_pi_weights ) 

        return user_pi_weights

        """
        import time
        tic = time.perf_counter()
        toc = time.perf_counter()
        print(f"Downloaded the tutorial in {toc - tic:0.4f} seconds")
        import ipdb; ipdb.set_trace()
        """
    

    def update_alg_old(self, new_data, t):
        import ipdb; ipdb.set_trace()
        
        # Incremental update
        rewards = new_data['reward'].to_numpy().reshape(-1,1)
        actions = new_data['action'].to_numpy().reshape(-1,1)
        action1prob = new_data['action1prob'].to_numpy().reshape(-1,1)
    
        base_states, treat_states = self.get_states(new_data)

        if self.action_centering:
            design = np.concatenate( [ base_states, action1prob * treat_states,
                        (actions-action1prob) * treat_states ], axis=1 )
        else:
            design = np.concatenate( [ base_states, actions * treat_states ], axis=1 )
        
        # Only include available data
        user_id = new_data['user_id'].to_numpy()
        if self.args.dataset_type == 'heartsteps':
            avail_bool = new_data['availability'].astype('bool')
            rewards_avail = rewards[ avail_bool ]
            design_avail = design[ avail_bool ]
            user_id_avail = user_id[avail_bool] 
        else:
            avail_bool = np.ones(rewards.shape)
            rewards_avail = rewards
            design_avail = design
            user_id_avail = user_id

        alg_dict = {
            "policy_last_t": t-1,
            "new_data" : new_data,  # data used to get updated est_params
            #"design" : design_avail,
            "total_obs" : self.all_policies[-1]['total_obs'] + len(new_data),
        }

        prev_policy_dict = self.all_policies[-1]
        RX = prev_policy_dict['RX'] + np.sum(rewards_avail * design_avail, axis=0)
        XX_add = np.einsum( 'ij,ik->jk', design_avail, design_avail )
        XX = prev_policy_dict['XX'] + XX_add
        inv_post_var = XX + np.linalg.inv( self.prior_var )
        post_var = np.linalg.inv( inv_post_var )
        prior_adj = np.matmul( np.linalg.inv(self.prior_var), self.prior_mean )
        post_mean = np.matmul( post_var, RX + prior_adj )

        col_names = self.state_feats+['action:'+x for x in self.treat_feats]
        all_user_id_set = prev_policy_dict["all_user_id"].copy()
        all_user_id_set.update( new_data["user_id"].to_numpy() )
        # all_user_id_set = set( all_prev_data['user_id'].to_numpy() ) 
        V_matrix = XX / len(all_user_id_set)
       
        # reconstruct posterior variance
        alg_dict["all_user_id"] = all_user_id_set
        alg_dict["n_users"] = len(all_user_id_set)
        alg_dict["post_mean"] = post_mean
        alg_dict["post_var"] = post_var
        alg_dict["col_names"] = col_names
        alg_dict["V_matrix"] = V_matrix
        alg_dict["XX"] = XX
        alg_dict["RX"] = RX
        alg_dict["intercept_val"] = V_matrix[0][0]
       
        inc_data_dict = {
                "rewards": rewards.flatten(),
                "actions": actions.flatten(),
                "action1probs": action1prob.flatten(),
                "base_states": base_states,
                "treat_states": treat_states,
                "avail": avail_bool.flatten(),
                "user_ids": user_id_avail,
                }
        alg_dict['inc_data_dict'] = inc_data_dict
        print(t, "rewards", rewards.shape)

        V_suffvec = var2suffvec(self, V_matrix)
        #varmatrix = suffvec2var(self, post_V_params)
        #post_V_params = get_utri(post_V)
        
        est_params = np.hstack( [ post_mean, V_suffvec ] )
        alg_dict["est_params"] = est_params
        
        self.all_policies.append(alg_dict)


